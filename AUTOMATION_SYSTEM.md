# System Automatyzacji ScraperÃ³w GPW2

## ğŸ¯ PrzeglÄ…d
Zaawansowany system harmonogramowania scraperÃ³w z konfigurowalnymi okresami czasowymi, wykrywaniem Å›wiÄ…t i automatycznÄ… analizÄ… AI.

## ğŸ“‹ Konfiguracja

### 1. Harmonogramy DomyÅ›lne

System zawiera 6 prekonfigurowanych harmonogramÃ³w:

#### News RSS
- **Market Hours**: Co 30min podczas godzin gieÅ‚dowych (7:00-18:00, pon-piÄ…)
- **Off Hours**: Co 2h poza godzinami gieÅ‚dowymi (18:01-6:59, codziennie)

#### Stock Prices
- **Live Trading**: Co 5min podczas sesji (9:00-17:30, pon-piÄ…)
- **EOD Update**: Codziennie po zamkniÄ™ciu (18:00-19:00, pon-piÄ…)

#### Calendar Events
- **Daily**: Codziennie rano (7:00-8:00, pon-piÄ…)

#### ESPI Reports
- **Business Hours**: Co 2h w godzinach biznesowych (8:00-18:00, pon-piÄ…)

### 2. Inteligentne Funkcje

âœ… **Session-aware timing** - automatyczne dostosowanie do godzin gieÅ‚dowych
âœ… **Holiday detection** - pomijanie polskich Å›wiÄ…t paÅ„stwowych
âœ… **Weekend handling** - konfigurowalne dziaÅ‚anie w weekendy
âœ… **Auto AI analysis** - automatyczna analiza nowych artykuÅ‚Ã³w
âœ… **Execution tracking** - peÅ‚ne logowanie wykonaÅ„ z metrykami
âœ… **Error handling** - obsÅ‚uga bÅ‚Ä™dÃ³w bez przerywania innych harmonogramÃ³w

## ğŸš€ Uruchomienie

### Setup (jednorazowy)
```bash
# Utworzenie domyÅ›lnych harmonogramÃ³w
python manage.py setup_scraping_schedules

# Lub reset istniejÄ…cych
python manage.py setup_scraping_schedules --reset
```

### Uruchamianie
```bash
# Sprawdzenie statusu
python manage.py run_scheduled_scrapers --status

# Uruchomienie wszystkich naleÅ¼nych zadaÅ„
python manage.py run_scheduled_scrapers

# Uruchomienie konkretnego typu
python manage.py run_scheduled_scrapers --type news_rss
python manage.py run_scheduled_scrapers --type stock_prices
python manage.py run_scheduled_scrapers --type calendar_events

# Uruchomienie konkretnego harmonogramu
python manage.py run_scheduled_scrapers --id 1
```

## ï¿½ PrzepÅ‚yw Danych

### News Articles Processing Pipeline

```
1. ğŸ“° SCRAPING PHASE (Batch)
   â”œâ”€â”€ RSS feeds fetched
   â”œâ”€â”€ Articles parsed & saved to database
   â””â”€â”€ All articles stored with source references

2. ğŸ¤– AI ANALYSIS PHASE (Incremental Batch)
   â”œâ”€â”€ Query: ai_classification__isnull=True
   â”œâ”€â”€ Process max 5 articles per session
   â”œâ”€â”€ Generate sentiment & industry analysis
   â”œâ”€â”€ Save analysis results to database
   â””â”€â”€ Mark articles as analyzed

3. ğŸ” CONTINUOUS PROCESSING
   â”œâ”€â”€ Next scraper run: new articles â†’ database
   â”œâ”€â”€ Next AI session: analyze remaining 5 articles
   â””â”€â”€ Repeat until all articles analyzed
```

**Kluczowe zalety:**
- âœ… **Oddzielone fazy** - scraping nie czeka na AI
- âœ… **Incremental** - kaÅ¼da sesja analizuje kolejnÄ… czÄ™Å›Ä‡
- âœ… **Fault tolerant** - bÅ‚Ä…d jednego artykuÅ‚u nie blokuje innych
- âœ… **Performance aware** - limit 5 artykuÅ‚Ã³w chroni przed timeout

### Crontab (zalecane)
```bash
# Dodaj do crontaba (crontab -e):
# Uruchamianie co 5 minut
*/5 * * * * cd /path/to/GPW2 && python manage.py run_scheduled_scrapers >> /var/log/scraping.log 2>&1

# Sprawdzanie statusu raz dziennie o 6:00
0 6 * * * cd /path/to/GPW2 && python manage.py run_scheduled_scrapers --status >> /var/log/scraping-status.log 2>&1
```

### Systemd Service
```ini
[Unit]
Description=GPW2 Scraping Scheduler
After=network.target

[Service]
Type=simple
User=gpw2
WorkingDirectory=/path/to/GPW2
ExecStart=/path/to/GPW2/venv/bin/python manage.py run_scheduled_scrapers --daemon
Restart=always
RestartSec=300

[Install]
WantedBy=multi-user.target
```

## ğŸ“Š Monitoring

### Status Check
```bash
python manage.py run_scheduled_scrapers --status
```

Pokazuje:
- LiczbÄ™ aktywnych harmonogramÃ³w
- LiczbÄ™ naleÅ¼nych zadaÅ„
- Ostatnie wykonania
- NastÄ™pne zaplanowane uruchomienia

### Logi WykonaÅ„
```python
# Django shell
from apps.core.models import ScrapingExecution
recent = ScrapingExecution.objects.all()[:10]
for exec in recent:
    print(f"{exec} - Duration: {exec.duration}s")
```

## âš™ï¸ Konfiguracja HarmonogramÃ³w

### Model: ScrapingSchedule

KaÅ¼dy harmonogram ma:
- **CzÄ™stotliwoÅ›Ä‡**: minuty, godziny, dni
- **Godziny aktywnoÅ›ci**: start/stop (np. 9:00-17:30)
- **Dni tygodnia**: pon-nie (osobno konfigurowalne)
- **Wykrywanie Å›wiÄ…t**: automatyczne pomijanie polskich Å›wiÄ…t
- **Konfiguracja scrapera**: JSON z parametrami

### PrzykÅ‚ad Tworzenia Custom Schedule

```python
from apps.core.models import ScrapingSchedule
from datetime import time

schedule = ScrapingSchedule.objects.create(
    name="Custom News - Frequent",
    scraper_type='news_rss',
    frequency_value=15,
    frequency_unit='minutes',
    active_hours_start=time(8, 0),
    active_hours_end=time(20, 0),
    monday=True,
    tuesday=True,
    wednesday=True,
    thursday=True,
    friday=True,
    saturday=False,
    sunday=False,
    scraper_config={
        'auto_analyze': True,
        'max_articles_per_run': 50
    }
)
schedule.calculate_next_run()
```

## ğŸ›ï¸ Panel Administracyjny

Harmonogramy sÄ… dostÄ™pne w Django Admin pod `/admin/core/scrapingschedule/`:

- **PodglÄ…d**: Status, nastÄ™pne uruchomienie, statystyki
- **Edycja**: Zmiana czÄ™stotliwoÅ›ci, godzin, dni
- **Aktywacja/Deaktywacja**: Åatwe wÅ‚Ä…czanie/wyÅ‚Ä…czanie
- **Historia**: Logi wszystkich wykonaÅ„

## ğŸš¨ RozwiÄ…zywanie ProblemÃ³w

### Schedule nie uruchamia siÄ™
```bash
# SprawdÅº czy jest aktywny
python manage.py shell -c "
from apps.core.models import ScrapingSchedule
schedule = ScrapingSchedule.objects.get(name='Nazwa Schedule')
print(f'Active: {schedule.is_active}')
print(f'Next run: {schedule.next_run}')
print(f'Should run now: {schedule.should_run_now()}')
"
```

### BÅ‚Ä™dy wykonania
```bash
# SprawdÅº ostatnie bÅ‚Ä™dy
python manage.py shell -c "
from apps.core.models import ScrapingExecution
failed = ScrapingExecution.objects.filter(success=False)[:5]
for exec in failed:
    print(f'{exec.schedule.name}: {exec.error_message}')
"
```

### Reset harmonogramÃ³w
```bash
# UsuÅ„ wszystkie i utwÃ³rz ponownie
python manage.py setup_scraping_schedules --reset
```

## ğŸ“ˆ Metryki

System automatycznie zbiera:
- **Czas wykonania** (duration)
- **Przetworzone elementy** (items_processed)
- **Utworzone elementy** (items_created)
- **Zaktualizowane elementy** (items_updated)
- **Komunikaty bÅ‚Ä™dÃ³w** (error_message)
- **SzczegÃ³Å‚y wykonania** (execution_details)

## ğŸ”® Funkcje Zaawansowane

### Automatyczna Analiza AI
- **Inteligentny przepÅ‚yw**: Najpierw batch scrapingu artykuÅ‚Ã³w, potem batch analizy AI
- **Incremental processing**: Analizuje tylko nieanalyzowane artykuÅ‚y (`ai_classification__isnull=True`)
- **Performance limit**: Maksymalnie 5 artykuÅ‚Ã³w na sesjÄ™ analizy
- **Continuous processing**: KaÅ¼de uruchomienie analizuje kolejnÄ… czÄ™Å›Ä‡ nieanalyzowanych
- **Graceful errors**: BÅ‚Ä™dy analizy nie przerywajÄ… procesu dla innych artykuÅ‚Ã³w
- **Market hours awareness**: Auto-analiza tylko podczas godzin gieÅ‚dowych (config: `auto_analyze: true`)
- **Off-hours mode**: Poza godzinami gieÅ‚dowymi scraping bez auto-analizy (config: `auto_analyze: false`)

### Wykrywanie ÅšwiÄ…t
- Automatycznie pomija polskie Å›wiÄ™ta paÅ„stwowe
- Konfigurowalne przez `respect_holidays=True/False`
- MoÅ¼na dodaÄ‡ custom dni wolne w metodzie `is_polish_holiday()`

### Session-aware Scheduling
- Automatycznie dostosowuje siÄ™ do godzin gieÅ‚dowych
- RÃ³Å¼ne czÄ™stotliwoÅ›ci dla rÃ³Å¼nych pÃ³r dnia
- Inteligentne calculation next_run uwzglÄ™dniajÄ…c weekendy

---

## âœ… Podsumowanie Implementacji

System automatyzacji scraperÃ³w jest w peÅ‚ni gotowy do produkcji z:

1. âœ… **6 prekonfigurowanych harmonogramÃ³w** z inteligentnymi interwaÅ‚ami
2. âœ… **PeÅ‚na automatyzacja** z session-aware timing i holiday detection
3. âœ… **Management commands** do kontroli i monitoringu
4. âœ… **Execution tracking** z metrykami i logowaniem bÅ‚Ä™dÃ³w
5. âœ… **Automatyczna analiza AI** nowych artykuÅ‚Ã³w
6. âœ… **Gotowe do produkcji** z instrukcjami crontab i systemd

Konfiguracja: **5min** | Uruchomienie: **1 command** | Monitoring: **Built-in**
